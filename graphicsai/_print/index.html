<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link rel="canonical" type="text/html" href="https://hamhuo.xyz/graphicsai/">
<link rel="alternate" type="application/rss&#43;xml" href="https://hamhuo.xyz/graphicsai/index.xml">
<meta name="robots" content="noindex, nofollow">


<link rel="shortcut icon" href="/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/favicons/android-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/favicons/android-192x192.png" sizes="192x192">

<title>Graphics and AI | 海阔集</title>
<meta name="description" content="Massey University Dr. Juncheng Liu
Under instruction of Dr.Liu i had an opprtunrty start my very early academic exploration.
3/6 实验室, 学术准备前的入门工作">
<meta property="og:url" content="https://hamhuo.xyz/graphicsai/">
  <meta property="og:site_name" content="海阔集">
  <meta property="og:title" content="Graphics and AI">
  <meta property="og:description" content="Massey University Dr. Juncheng Liu
Under instruction of Dr.Liu i had an opprtunrty start my very early academic exploration.
3/6 实验室, 学术准备前的入门工作">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="website">

  <meta itemprop="name" content="Graphics and AI">
  <meta itemprop="description" content="Massey University Dr. Juncheng Liu
Under instruction of Dr.Liu i had an opprtunrty start my very early academic exploration.
3/6 实验室, 学术准备前的入门工作">
  <meta itemprop="dateModified" content="2025-03-08T12:51:48+08:00">
  <meta itemprop="wordCount" content="22">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Graphics and AI">
  <meta name="twitter:description" content="Massey University Dr. Juncheng Liu
Under instruction of Dr.Liu i had an opprtunrty start my very early academic exploration.
3/6 实验室, 学术准备前的入门工作">
<link rel="preload" href="/scss/main.min.6c18e25eadfb13d8abc6b872960e12aab94122cfeded66e3b312c962ee366f20.css" as="style" integrity="sha256-bBjiXq37E9irxrhylg4SqrlBIs/t7WbjsxLJYu42byA=" crossorigin="anonymous">
<link href="/scss/main.min.6c18e25eadfb13d8abc6b872960e12aab94122cfeded66e3b312c962ee366f20.css" rel="stylesheet" integrity="sha256-bBjiXq37E9irxrhylg4SqrlBIs/t7WbjsxLJYu42byA=" crossorigin="anonymous">
<script
  src="https://code.jquery.com/jquery-3.7.1.min.js"
  integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g=="
  crossorigin="anonymous"></script>
<script defer
  src="https://unpkg.com/lunr@2.3.9/lunr.min.js"
  integrity="sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli"
  crossorigin="anonymous"></script>

  </head>
  <body class="td-section">
    <header>
      <nav class="td-navbar js-navbar-scroll" data-bs-theme="dark">
<div class="container-fluid flex-column flex-md-row">
  <a class="navbar-brand" href="/"><span class="navbar-brand__logo navbar-logo"><svg width="800" height="800" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M23.3 8.40007 21.82 6.40008C21.7248 6.27314 21.6008 6.17066 21.4583 6.10111 21.3157 6.03156 21.1586 5.99693 21 6.00008H11.2C11.0555 6.00007 10.9128 6.03135 10.7816 6.09177 10.6504 6.15219 10.5339 6.24031 10.44 6.35007L8.71998 8.35008C8.57227 8.53401 8.49435 8.76424 8.49998 9.00008V16.2901C8.50262 18.0317 9.19567 19.7013 10.4272 20.9328c1.2316 1.2316 2.9011 1.9246 4.6428 1.9273h1.86C18.6716 22.8574 20.3412 22.1644 21.5728 20.9328c1.2315-1.2315 1.9245-2.9011 1.9272-4.6427V9.00008C23.5 8.7837 23.4298 8.57317 23.3 8.40007z" fill="#ffcc80"/><path d="M29.78 28.38l-4-5C25.664 23.2321 25.5086 23.1198 25.3318 23.0562 25.1549 22.9925 24.9637 22.98 24.78 23.02L16 25 7.21999 23C7.03632 22.96 6.84507 22.9725 6.6682 23.0362 6.49133 23.0998 6.33598 23.2121 6.21999 23.36l-4 5C2.10392 28.5064 2.03116 28.6823 2.00995 28.8679 1.98874 29.0534 2.01993 29.2413 2.09999 29.41 2.17815 29.5839 2.3044 29.7319 2.46385 29.8364 2.62331 29.9409 2.80933 29.9977 2.99999 30H29C29.1885 29.9995 29.373 29.9457 29.5322 29.8448 29.6914 29.744 29.8189 29.6002 29.9 29.43 29.98 29.2613 30.0112 29.0734 29.99 28.8879 29.9688 28.7023 29.8961 28.5264 29.78 28.38z" fill="#01579b"/><path d="M29.29 6.00003l-13-4C16.0999 1.95002 15.9001 1.95002 15.71 2.00003l-13 4c-.21258.06419-.39774.19732-.5263.37838C2.05515 6.55947 1.99052 6.77817 2 7.00003 1.9917 7.22447 2.0592 7.44518 2.19163 7.62659c.13242.1814.32207.31295.53837.37344L15.73 11.6C15.906 11.6534 16.094 11.6534 16.27 11.6l13-3.59997C29.4863 7.93954 29.6759 7.80799 29.8084 7.62659 29.9408 7.44518 30.0083 7.22447 30 7.00003 30.0095 6.77817 29.9448 6.55947 29.8163 6.37841 29.6877 6.19735 29.5026 6.06422 29.29 6.00003z" fill="#01579b"/><path d="M11.22 6C11.0756 5.99999 10.9328 6.03127 10.8016 6.09169 10.6704 6.15211 10.5539 6.24023 10.46 6.35l-1.72 2C8.58509 8.53114 8.49998 8.76166 8.5 9v7.29C8.50264 18.0317 9.19569 19.7012 10.4272 20.9328c1.2316 1.2315 2.9011 1.9246 4.6428 1.9272H16V6H11.22z" fill="#ffe0b2"/><path d="M7.21999 23C7.03632 22.96 6.84507 22.9725 6.6682 23.0362 6.49133 23.0998 6.33598 23.2121 6.21999 23.36l-4 5C2.10392 28.5064 2.03116 28.6823 2.00995 28.8679 1.98874 29.0534 2.01993 29.2413 2.09999 29.41 2.17815 29.5839 2.3044 29.7319 2.46385 29.8364 2.62331 29.9409 2.80933 29.9977 2.99999 30H16V25L7.21999 23z" fill="#0277bd"/><path d="M15.71 2.00002l-13 4C2.49742 6.06422 2.31226 6.19734 2.1837 6.3784c-.12855.18107-.19318.39977-.1837.62162C1.9917 7.22447 2.0592 7.44518 2.19163 7.62658 2.32405 7.80799 2.5137 7.93954 2.73 8.00002L15.73 11.6C15.8194 11.6146 15.9106 11.6146 16 11.6V2.00002C15.9039 1.98469 15.8061 1.98469 15.71 2.00002z" fill="#0277bd"/><path d="M2.73 8.00003l5.77 1.56V16.29C8.50264 18.0317 9.19569 19.7013 10.4272 20.9328c1.2316 1.2315 2.9011 1.9246 4.6428 1.9272h1.86C18.6717 22.8574 20.3412 22.1643 21.5728 20.9328c1.2315-1.2315 1.9246-2.9011 1.9272-4.6428V9.56003l5.77-1.56C29.4863 7.93954 29.6759 7.80799 29.8084 7.62659 29.9408 7.44518 30.0083 7.22447 30 7.00003 30.0095 6.77817 29.9448 6.55947 29.8163 6.37841 29.6877 6.19735 29.5026 6.06422 29.29 6.00003l-13-4C16.0999 1.95002 15.9001 1.95002 15.71 2.00003l-13 4c-.21258.06419-.39774.19732-.5263.37838C2.05515 6.55947 1.99052 6.77817 2 7.00003 1.9917 7.22447 2.0592 7.44518 2.19163 7.62659c.13242.1814.32207.31295.53837.37344zM21.5 16.29C21.4974 17.5013 21.015 18.6621 20.1586 19.5186 19.3021 20.3751 18.1412 20.8574 16.93 20.86H15.07C13.8588 20.8574 12.6979 20.3751 11.8414 19.5186 10.985 18.6621 10.5026 17.5013 10.5 16.29V10.11l5.23 1.45C15.906 11.6134 16.094 11.6134 16.27 11.56l5.23-1.45v6.18zM16 4.05003l9.44 2.95-9.44 2.56-9.44-2.56 9.44-2.95z" fill="#263238"/><path d="M25.78 23.38C25.664 23.2321 25.5086 23.1198 25.3318 23.0562 25.1549 22.9925 24.9637 22.98 24.78 23.02L16 25 7.21999 23C7.03632 22.96 6.84507 22.9725 6.6682 23.0362 6.49133 23.0998 6.33598 23.2121 6.21999 23.36l-4 5C2.10392 28.5064 2.03116 28.6823 2.00995 28.8679 1.98874 29.0534 2.01993 29.2413 2.09999 29.41 2.17815 29.5839 2.3044 29.7319 2.46385 29.8364 2.62331 29.9409 2.80933 29.9977 2.99999 30H29C29.1885 29.9995 29.373 29.9457 29.5322 29.8448 29.6914 29.744 29.8189 29.6002 29.9 29.43 29.98 29.2613 30.0112 29.0734 29.99 28.8879 29.9688 28.7023 29.8961 28.5264 29.78 28.38l-4-5zM5.07999 28l2.31-2.89L15.78 27C15.9251 27.0299 16.0748 27.0299 16.22 27l8.39-1.87L26.92 28H5.07999z" fill="#263238"/></svg></span><span class="navbar-brand__name">海阔集</span></a>
  <div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
    <ul class="navbar-nav">
      <li class="nav-item">
        <a class="nav-link" href="/codewar/"><span>Code War</span></a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="/school/"><span>Course</span></a>
      </li>
      <li class="nav-item">
        <a class="nav-link active" href="/graphicsai/"><span>Graphics and AI</span></a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="/graphicsai/torch/"><span>Pytorch basics tutorials</span></a>
      </li>
      <li class="td-light-dark-menu nav-item dropdown">
        <svg xmlns="http://www.w3.org/2000/svg" class="d-none">
  <symbol id="check2" viewBox="0 0 16 16">
    <path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"/>
  </symbol>
  <symbol id="circle-half" viewBox="0 0 16 16">
    <path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"/>
  </symbol>
  <symbol id="moon-stars-fill" viewBox="0 0 16 16">
    <path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"/>
    <path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"/>
  </symbol>
  <symbol id="sun-fill" viewBox="0 0 16 16">
    <path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"/>
  </symbol>
</svg>

<button class="btn btn-link nav-link dropdown-toggle d-flex align-items-center"
        id="bd-theme"
        type="button"
        aria-expanded="false"
        data-bs-toggle="dropdown"
        data-bs-display="static"
        aria-label="Toggle theme (auto)">
  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg>
</button>
<ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
  <li>
    <button type="button" class="dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
      <svg class="bi me-2 opacity-50"><use href="#sun-fill"></use></svg>
      Light
      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg>
    </button>
  </li>
  <li>
    <button type="button" class="dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
      <svg class="bi me-2 opacity-50"><use href="#moon-stars-fill"></use></svg>
      Dark
      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg>
    </button>
  </li>
  <li>
    <button type="button" class="dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
      <svg class="bi me-2 opacity-50"><use href="#circle-half"></use></svg>
      Auto
      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg>
    </button>
  </li>
</ul>

      </li>
      </ul>
  </div>
  <div class="d-none d-lg-block">
    <div class="td-search td-search--offline">
  <div class="td-search__icon"></div>
  <input
    type="search"
    class="td-search__input form-control"
    placeholder="Search this site…"
    aria-label="Search this site…"
    autocomplete="off"
    
    data-offline-search-index-json-src="/offline-search-index.4c985dba55cb853653b69248d9c56683.json"
    data-offline-search-base-href="/"
    data-offline-search-max-results="10"
  >
</div>
  </div>
</div>
</nav>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 ps-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This is the multi-page printable view of this section.
<a href="#" onclick="print();return false;">Click here to print</a>.
</p><p>
<a href="/graphicsai/">Return to the regular view of this page</a>.
</p>
</div>



<h1 class="title">Graphics and AI</h1>





    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-1ac861205bc9f126a76edfb2660c4136">A simple network to classify handwritten digits</a></li>


    
  
    
    
	
<li>2: <a href="#pg-32bcf2e48ffc875b36f6b00765991f31">Learning with gradient descent</a></li>


    
  
    
    
	
<li>3: <a href="#pg-d6aa585e4ed0abbe857b9a88f8fe71c1">Neutral Network</a></li>


    
  
    
    
	
<li>4: <a href="#pg-9df916075c8932af6a8fa84649921c2d">The architecture of neural networks</a></li>


    
  
    
    
	
<li>5: <a href="#pg-d44b795454d5eab591882fea1f2920af"></a></li>


    
  
    
    
	
<li>6: <a href="#pg-df192624770c0d325fbe1f04e6e5c4e9"></a></li>


    
  
    
    
	
<li>7: <a href="#pg-5eadbe77d75e772bd85af73fd5d3ede9">Pytorch</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>7.1: <a href="#pg-c50f9bec0d0291a4d85a469b3d967a6a">Quick Start</a></li>


    
  

    </ul>
    
  

    </ul>


<div class="content">
      <p>Massey University
Dr. Juncheng Liu</p>
<p>Under instruction of Dr.Liu i had an opprtunrty start my very early academic exploration.</p>
<hr>
<p>3/6 实验室, 学术准备前的入门工作</p>

</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-1ac861205bc9f126a76edfb2660c4136">1 - A simple network to classify handwritten digits</h1>
    
	<p>OK, let&rsquo;s get into the real problem  handwriting recognition</p>
<p>First, we&rsquo;d like a way of breaking an image containing many digits into a sequence of separate images, each containing a single digit.</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250308021049829.png" alt="image.png">
<img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250308021056145.png" alt="image.png"></p>
<p>We&rsquo;ll focus on writing a program to solve the second problem</p>
<p>We do this because it turns out that the segmentation problem is not so difficult to solve, once you have a good way of classifying individual digits.</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250309130426922.png" alt="image.png"></p>
<p>The first layer is input layer.</p>
<p>For simplicity I&rsquo;ve omitted most of the 784784 input neurons in the diagram above.</p>
<p>The second layer is hidden layer We denote the number of neurons in this hidden layer by n</p>
<p>The output layer contain 10 neurons， we number the output neurons from 0 through 9, and figure out which neuron has the highest activation value.</p>
<blockquote>
<p>why we need three layer to reco digit instead of two?
what&rsquo;s the role of hidden and output layer? why we need a outputlayer?
how the weight comes up?</p></blockquote>
<p>to explain those, let&rsquo;s  think about what the neural network is doing from first principles, we had input of every pixels, and output layer add all evidence and decide true or false</p>
<p>that&rsquo;s quite simple, but where the evidence comes from?</p>
<p>hidden layers provided evidence, and let&rsquo;s concentrate on the first hidden neurons
detects whether or not an image like the following is present</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250309134539159.png" alt="image.png"></p>
<p>It can do this by heavily weighting input pixels which overlap with the image, and only lightly weighting the other inputs.</p>
<p>the same way, if other neural fired.
<img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250309134837751.png" alt="image.png"></p>
<p>we can get an 0 from output
<img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250309134902210.png" alt="image.png"></p>
<h4 id="exercise">Exercise<a class="td-heading-self-link" href="#exercise" aria-label="Heading self-link"></a></h4>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><p>There is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below.</p>
<p>Find a set of weights and biases for the new output layer. Assume that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least 0.99, and incorrect outputs have activation less than 0.01</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250309154316925.png" alt="image.png"></p>
<p>we had input layer which hold pixels in grayscale, through hidden layer, we get output from output layer, as said, output closer to 1 can indicate the number. now add new output layer which convert old output to 0 or 1.
we had map as below</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250309185657564.png" alt="image.png"></p>
<p>For the first neuron, only digits 8 or 9 can be activated. This means that neurons 8 and 9 in the old input layer had a greater weight influence on the first neuron in the new output layer.</p>
<p>The same principle applies to other neurons.<br>
But how do we determine the actual values of the biases and weights?</p>
<p>That doesn’t matter—we don’t need to manually design a set of weights or biases. Instead, we should understand that a node in the old output layer that is closer to 1 can be interpreted as having a positive influence. This influence is then mapped to the corresponding node in the new output layer.</p>
<p>for example we have neurons 8 closer to 1. and we fire first node in new output layer</p>
<p>The weights and biases are actually optimized by the system itself using a methodology called <a href="/graphicsai/gradientdescent/">gradient descent</a>.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-32bcf2e48ffc875b36f6b00765991f31">2 - Learning with gradient descent</h1>
    
	<p> <em><strong>our goal in training a neural network is to find weights and biases which minimize the quadratic cost function C(w,b)</strong></em></p>
<p>We already kown how neural network is functioned. an input layer and outputlayer and hidden layer.</p>
<p>To let network learn itself, we need some data. Like digits</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250312200233941.png" alt="image.png"></p>
<p>Actually, these digits come from a well-known training dataset called the <strong>MNIST dataset</strong>, which contains tens of thousands of scanned images of handwritten digits along with their correct classifications.</p>
<p>The MNIST data is divided into two parts: <strong>Training</strong> and <strong>Test</strong> datasets. We primarily use the training dataset to improve the model&rsquo;s accuracy and the test dataset to evaluate the model&rsquo;s performance.</p>
<p>For simplicity, let&rsquo;s simplify the process. In linear algebra, we know that multiple values can define a &lsquo;dot&rsquo; in space. So, we transform the input layer into a vector (which we can think of as a dot), and then use a function to map it to the target value <strong>y</strong>.</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250312200754080.png" alt="image.png"></p>
<p>In this case, <strong>y</strong> is the output, which ranges from 0 to 9. This means <strong>y</strong> is a 10-dimensional vector.</p>
<p>As for <strong>x</strong>, we simplify the entire input layer into a single notation: <strong>x</strong>. Let me explain further:</p>
<p><strong>x</strong> is a 28×28=784-dimensional vector. We define each pixel as an input neuron, and the vector represents the grayscale value of a single pixel in the image.</p>
<p>So, in this process, we map the 784-dimensional vector (input) into a 10-dimensional vector (output).</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313104456461.png" alt="image.png"></p>
<p>turning a row vector into an ordinary (column) vector.</p>
<p>Actially, we want a method lets us find weights and biases, so that the output from the network approximates y(x) for all training inputs x.</p>
<p>To achieve that we introduce <em><strong>cost function</strong></em></p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313104915863.png" alt="image.png"></p>
<p>a simple explain here</p>
<p>w denotes the collection of all weights in the network, b all the biases</p>
<p>n is the total number of training inputs</p>
<p>a is the vector of outputs from the network when x is input, and the sum is over all training inputs</p>
<p>call C the <em>quadratic</em> cost function or <em>mean squared error</em> or just <em>MSE</em></p>
<p>C(w,b) is non-negative and mainly we can considerd as our training algorithm has done a good job if it can find weights and biases so that C(w,b)≈0</p>
<p>OK, why we not just simply let y(x) deduct a?</p>
<p>The problem with that is that the number of images correctly classified is not a smooth function of the weights and biases in the network.</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313111308753.png" alt="image.png"></p>
<p>we can barly get infomation from that, That makes it difficult to figure out how to change the weights and biases to get improved performance. If we instead use a smooth cost function like the quadratic cost it turns out to be easy to figure out how to make small changes in the weights and biases so as to get an improvement in the cost.</p>
<p>C(v) v This could be any real-valued function of many variables, v=v1,v2,…v=v1,v2,….</p>
<p> To minimize C(v) it helps to imagine C as a function of just two variables, which we&rsquo;ll call v1 and v2
 
<img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313111604981.png" alt="image.png"></p>
<p>Fuction above is sample, the main idea is to find the minimize value However,  calculus doesn&rsquo;t work. beacuse we will have tons of variables in function far more than 2</p>
<p>there is a beautiful analogy which suggests an algorithm which works pretty well.
 
We could do this simulation simply by computing derivatives (and perhaps some second derivatives) of C - those derivatives would tell us everything we need to know about the local &ldquo;shape&rdquo; and this is nothing with physics we just trying to find where derivatives stop.</p>
<p>let&rsquo;s think about what happens when we move the ball a small amount Δv1 in the v1 direction, and a small amount Δv2 in the v2 direction.</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313112203030.png" alt="image.png"></p>
<p>with liner alog let&rsquo;s try seprated changes in v to changes in C</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313112405514.png" alt="image.png"></p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313112415788.png" alt="image.png"></p>
<p>ΔC can be rewritten as</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313112456114.png" alt="image.png"></p>
<p>what&rsquo;s really exciting about the equation is that it lets us see how to choose ΔvΔv so as to make ΔC negative. do not forget we keep tracing down the shape</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313112623339.png" alt="image.png"></p>
<p>where η is a small, positive parameter，we called learning rate.</p>
<p>we indicate position by</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313113000335.png" alt="image.png"></p>
<p>which will show in graph like &ldquo;falling down&rdquo;</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313113154483.png" alt="image.png"></p>
<p>But we have one problem here, we need to choose the learning rate to be small enough and approach good approximation. ==If we don&rsquo;t, we might end up with ΔC&gt;0. If we don&rsquo;t, we might end up with ΔC&gt;0.==  At the same time, we don&rsquo;t want η to be too small, since that will make the changes Δv tiny</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313114505482.png" alt="image.png"></p>
<p>with speed 0.03 we used 2495 steps and output curve relativly smooth</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250313114640863.png" alt="image.png"></p>
<p>with speed 3 output curve is more abstract and reach a mini spot needs lucky</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d6aa585e4ed0abbe857b9a88f8fe71c1">3 - Neutral Network</h1>
    <div class="lead">introduce Perceptrons and sigmoid</div>
	<blockquote>
<p>refer:
<a href="http://neuralnetworksanddeeplearning.com/chap1.html">Using neural nets to recognize handwritten digits</a></p></blockquote>
<p><em><strong>patience in the face of such frustration is the only way to truly understand and internalize a subject.</strong></em>  FIND YOUR PROJECT</p>
<h4 id="overview">Overview<a class="td-heading-self-link" href="#overview" aria-label="Heading self-link"></a></h4>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><h6 id="recognize-hand-digits">recognize hand digits<a class="td-heading-self-link" href="#recognize-hand-digits" aria-label="Heading self-link"></a></h6>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><p>Let&rsquo;s start from a basic task,  recognize those digits
 we don&rsquo;t usually appreciate how tough a problem our visual systems solve.
 
 write a computer program to recognize digits like those above
 
 try to make such rules precise will got tou confuse</p>
<h6 id="neural-networks">Neural networks<a class="td-heading-self-link" href="#neural-networks" aria-label="Heading self-link"></a></h6>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><p>The idea is to take a large number of handwritten digits</p>
<p> develop a system which can learn from those training examples</p>
<p>artificial neuron called a <em>perceptron</em>. the main neuron model used is one called the <em>sigmoid neuron</em></p>
<h6 id="write-a-computer-program-implementing-a-neural-network">write a computer program implementing a neural network<a class="td-heading-self-link" href="#write-a-computer-program-implementing-a-neural-network" aria-label="Heading self-link"></a></h6>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><p>ocusing on handwriting recognition because it&rsquo;s an excellent prototype problem for learning about neural networks in general.</p>
<h6 id="improve-accuracy">improve accuracy<a class="td-heading-self-link" href="#improve-accuracy" aria-label="Heading self-link"></a></h6>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><h4 id="perceptrons">Perceptrons<a class="td-heading-self-link" href="#perceptrons" aria-label="Heading self-link"></a></h4>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><pre><code>a perceptrons is a device to add evidence to make decisions
</code></pre>
<p>perceptron is that it&rsquo;s a device that makes decisions by weighing up evidence</p>
<p> ###### <strong>how do perceptrons work?</strong>
 
 ##### algebraic terms
 
A perceptron takes several binary inputs, x1,x2,…x1,x2,…, and produces <em><strong>a single binary output</strong></em>:</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307000017096.png" alt="image.png"></p>
<p>Rosenblatt use <em>weights</em> the <em><strong>importance</strong></em> of the respective inputs to the output
The neuron&rsquo;s output, 0 or 1, is determined by whether the weighted sum  is less than or greater than some <em>threshold value</em>.
 
<img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307000217496.png" alt="image.png"></p>
<p>A way you can think about the perceptron is that it&rsquo;s a device that
<strong>makes decisions by weighing up evidence.</strong></p>
<p>By varying the weights and the threshold, we can get different models of decision-making.</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307000649385.png" alt="image.png"></p>
<p>we&rsquo;ll call the first <em>layer</em> of perceptrons - is making three very simple decisions, by weighing the input evidence.</p>
<p> In this way a perceptron in the second layer can make a decision at a more complex and more abstract level than perceptrons in the first layer.</p>
<p>In this way, a many-layer network of perceptrons can engage in sophisticated decision making.</p>
<blockquote>
<p>In fact, they&rsquo;re still single output. The multiple output arrows are merely a useful way of indicating that the output from a perceptron is being used as the input to several other perceptrons.</p></blockquote>
<p>simplify one
The first change is to write </p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307002101970.png" alt="image.png">
as a dot product</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307002747189.png" alt="image.png"></p>
<p> w and x are vectors whose components are the weights and inputs</p>
<p> move the threshold to the other side of the inequality, and to replace it by what&rsquo;s known as the perceptron&rsquo;s <em>bias</em></p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307003222826.png" alt="image.png"></p>
<p>bias can be seen as a way to measure how easy  it is to get the perceptron to output a 1</p>
<blockquote>
<p>the bias is a measure of how easy it is to get the perceptron to <em>fire</em>.</p></blockquote>
<p> it leads to further notational simplifications. Because of this, in the remainder of the book we won&rsquo;t use the threshold, we&rsquo;ll always use the bias.</p>
<p> ###### <em><strong>compute the elementary logical functions</strong></em>
 suppose we have a perceptron with two inputs, each with weight −2, and an overall bias of 3.
 <img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307004135602.png" alt="image.png">
the input 1 produces output 0,
nputs 1 and 0 produce output 1
<code>NAND</code> gate!</p>
<p>we can use networks of perceptrons to compute <em>any</em> logical function at all.</p>
<blockquote>
<p>Actually, in neural networks, a neuron has <strong>only one output</strong>, which is then <strong>broadcasted</strong> to all its outgoing connections.</p></blockquote>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307005202624.png" alt="image.png"></p>
<p>for encoding input, we define a input layer, This notation for input perceptrons, in which we have an output, but no inputs. better to consider special units which are simply defined to output the desired values.</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307012809931.png" alt="image.png"></p>
<p>It turns out that we can devise <em>learning algorithms</em> which can automatically tune the weights and biases of a network of artificial neurons.</p>
<h4 id="sigmoid-neurons">Sigmoid neurons<a class="td-heading-self-link" href="#sigmoid-neurons" aria-label="Heading self-link"></a></h4>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><p>we already know that our neural networks can simply learn to solve problems, automatically tune the weights and biases of a network of artificial neurons.</p>
<p>This tuning happens in response to external stimuli, without direct intervention by a programmer.</p>
<p>how can we devise such algorithms for a neural network?</p>
<p>before design ourself, let&rsquo;s see how network behave when a small change happen</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307014434244.png" alt="image.png"></p>
<p>a small change will cause a small corresponding change in the output</p>
<p>this property will make learning possible what we need is to make small change and let output closer to actual value.</p>
<p>But the problem is, when small change happens. output from perceptron more likely to flip like 0 -&gt; 1, That flip may then cause the output overcontrol and unpredictable</p>
<p>That&rsquo;s where sigmoid neuron comes</p>
<p>We can overcome this problem by introducing a new type of artificial neuron called a <em>sigmoid</em> neuron</p>
<p>Sigmoid neurons are similar to perceptrons, but modified so that small changes in their weights and bias cause only a small change in their output.</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307094009408.png" alt="image.png"></p>
<p>output instead of 0 and 1 is <img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307094203315.png" alt="image.png"></p>
<p>where σ is called the <em>sigmoid function</em> or <em>logistic neurons</em></p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307094313685.png" alt="image.png"></p>
<p>we put it explicaitly</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307094447546.png" alt="image.png"></p>
<blockquote>
<p>we can take the algebraic form as a technical detail than barrier of
understatnding</p></blockquote>
<h6 id="the-similarity-to-the-perceptron-model">the similarity to the perceptron model<a class="td-heading-self-link" href="#the-similarity-to-the-perceptron-model" aria-label="Heading self-link"></a></h6>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307100155574.png" alt="image.png"></p>
<p>when <em>z</em> is large and positive, the output from the sigmoid neuron is approximately 1, On the Other hand the output is approximately 0</p>
<p>the behaviour of a sigmoid neuron also closely approximates a perceptron. Only z has  a modest size that there&rsquo;s much deviation from the perceptron model.</p>
<blockquote>
<p>Indeed, it&rsquo;s the smoothness of the σσ function that is the crucial fact, not its detailed form.</p></blockquote>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307234525293.png" alt="image.png"></p>
<p>and we have another version</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250307234603959.png" alt="image.png"></p>
<p>In that case, the the sigmoid neuron would <em>be</em> a <code>perceptron</code> since the output would be 1 or 0 (we ignore modest  value) so sigmoid is basicly smoother version of perceptron</p>
<p> Indeed, it&rsquo;s the smoothness of the σσ function that is the crucial fact</p>
<p>we already know <img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250308000136586.png" alt="image.png"> can be approximated by</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250308000156116.png" alt="image.png"></p>
<blockquote>
<p>Don&rsquo;t panic !!! Only the shape of fuction matters, actually we will talk more for other activation function</p>
<p>where the output is f(w⋅x+b) for some other <em>activation function</em> f(⋅)</p></blockquote>
<p>Δoutput is a <em>linear function</em> of the changes Δwj and Δb in the weights and bias</p>
<p>This linearity makes it easy to choose small changes in the weights and biases to achieve any desired small change in the output.</p>
<p>So <code>sigmod simulating perceptrons</code> make it much easier to figure out how changing the weights and biases will change the output.</p>
<p>Anyway, one big difference between perceptrons and sigmoid neurons is that sigmoid neurons don&rsquo;t just output 0 or 1.</p>
<h4 id="exercises">Exercises<a class="td-heading-self-link" href="#exercises" aria-label="Heading self-link"></a></h4>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><p><strong>Sigmoid neurons simulating perceptrons, part I</strong></p>
<p>Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, c&gt;0. Show that the behaviour of the network doesn&rsquo;t change.</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250308003301064.png" alt="20250308_003235.jpg"></p>
<p><strong>Sigmoid neurons simulating perceptrons, part II</strong></p>
<p>Suppose we have the same setup as the last problem - a network of perceptrons.
Suppose also that the overall input to the network of perceptrons has been chosen.
We won&rsquo;t need the actual input value, we just need the input to have been fixed.</p>
<p>Suppose the weights and biases are such that  <img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250308003502217.png" alt="image.png">for the input x to any particular perceptron in the network.
Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant c&gt;0.</p>
<p>Show that in the limit as c→∞ the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons.</p>
<p>How can this fail when w⋅x+b=0 for one of the perceptrons?</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250308005357594.png" alt="20250308_005335.jpg"></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9df916075c8932af6a8fa84649921c2d">4 - The architecture of neural networks</h1>
    <div class="lead">we will forcus on<code>feedforward</code> network which contains input, hidden, output layers with no loop</div>
	<p>neural network have three layers , input layer , output layer and &rsquo;not an input or output layer'</p>
<p>again, we have an network</p>
<p><img src="https://raw.githubusercontent.com/hamhuo-hub/HamPic/img/img/20250308014518176.png" alt="image.png"></p>
<p>the leftmost layer is input neurons the rightmost is outputneurons
The middle layer is called a <em>hidden layer</em> t really means nothing more than &ldquo;not an input or an output&rdquo;.</p>
<p>input and output often strightforword, while there is often an art to the design of the hidden layers.</p>
<blockquote>
<p>There are some design heuristics for the hidden layers. we are not included here</p></blockquote>
<p>until now, the output from one layer is used as input to the next layer is called <code>feedforward</code> network  information is always fed forward, no loop acceptable</p>
<blockquote>
<p><a href="http://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a> allow such loop, They&rsquo;re much closer in spirit to how our brains work than feedforward networks.</p></blockquote>
<blockquote>
<p><a href="/graphicsai/simpleclassifynet/">simpleclassifynet</a></p></blockquote>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d44b795454d5eab591882fea1f2920af">5 - </h1>
    
	<h1 id="ainote">AINote<a class="td-heading-self-link" href="#ainote" aria-label="Heading self-link"></a></h1>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><p>a remote repository of Massey</p>
<p>Reference:</p>
<p><a href="http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent">http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent</a></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-df192624770c0d325fbe1f04e6e5c4e9">6 - </h1>
    
	
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5eadbe77d75e772bd85af73fd5d3ede9">7 - Pytorch</h1>
    
	<ul>
<li>Pytorch basics tutorials:</li>
</ul>
<p>Learn the Basics — PyTorch Tutorials 2.6.0+cu124 documentation
pytorch.org
Most machine learning workflows involve working with data, creating models, optimizing model parameters, and saving the trained models.</p>
<ul>
<li>Training an image classifier using CNN</li>
</ul>
<p>Deep Learning with PyTorch: A 60 Minute Blitz — PyTorch Tutorials 2.6.0+cu124 documentation
pytorch.org</p>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-c50f9bec0d0291a4d85a469b3d967a6a">7.1 - Quick Start</h1>
    <div class="lead">runs through the API for common tasks in machine learning</div>
	<p>PyTorch is an open-source deep learning framework that’s known for its flexibility and ease-of-use.</p>
<h5 id="python-grammer">Python Grammer<a class="td-heading-self-link" href="#python-grammer" aria-label="Heading self-link"></a></h5>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><blockquote>
<p>understand that as a playground with instructure.
define a function use <em><strong>def</strong></em> , function in python can return multiable values
no need of type <em><strong>attribute</strong></em></p></blockquote>
<p>In python list is reinitializable, and turple is fixed and unchangable</p>
<div class="highlight"><pre tabindex="0" style="color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81c8be">import</span> <span style="color:#ef9f76">math</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ca9ee6">def</span> <span style="color:#8caaee">move</span>(x, y, step, angle<span style="color:#99d1db;font-weight:bold">=</span><span style="color:#ef9f76">0</span>):
</span></span><span style="display:flex;"><span>    nx <span style="color:#99d1db;font-weight:bold">=</span> x <span style="color:#99d1db;font-weight:bold">+</span> step <span style="color:#99d1db;font-weight:bold">*</span> math<span style="color:#99d1db;font-weight:bold">.</span>cos(angle)
</span></span><span style="display:flex;"><span>    ny <span style="color:#99d1db;font-weight:bold">=</span> y <span style="color:#99d1db;font-weight:bold">-</span> step <span style="color:#99d1db;font-weight:bold">*</span> math<span style="color:#99d1db;font-weight:bold">.</span>sin(angle)
</span></span><span style="display:flex;"><span>    <span style="color:#ca9ee6">return</span> nx, ny
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#99d1db;font-weight:bold">&gt;&gt;&gt;</span> x, y <span style="color:#99d1db;font-weight:bold">=</span> move(<span style="color:#ef9f76">100</span>, <span style="color:#ef9f76">100</span>, <span style="color:#ef9f76">60</span>, math<span style="color:#99d1db;font-weight:bold">.</span>pi <span style="color:#99d1db;font-weight:bold">/</span> <span style="color:#ef9f76">6</span>)
</span></span><span style="display:flex;"><span><span style="color:#99d1db;font-weight:bold">&gt;&gt;&gt;</span> <span style="color:#99d1db">print</span>(x, y)
</span></span><span style="display:flex;"><span><span style="color:#ef9f76">151.96152422706632</span> <span style="color:#ef9f76">70.0</span>
</span></span></code></pre></div><p>A huge improve is Python supports default para. In tons of value need to write value, we can ignore the fixed values and only trans uniqe ones.</p>
<blockquote>
<p>WARN:
python 的默认参数是在方法外部传入的, 一定注意Python是引用传参, 默认参数必须指向不变对象！</p></blockquote>
<p>Written in Python, it’s relatively easy for most machine learning developers to learn and use.</p>
<p>For data, we have two primitives in toch</p>
<p><em><strong>torch.utils.data.DataLoader</strong></em>
<em><strong>torch.utils.data.Dataset</strong></em></p>
<p>Dataset include a bunch of samples and corresponding lables.</p>
<blockquote>
<p>Note here In python <em><strong>map</strong></em> called <em><strong>dict</strong></em></p></blockquote>
<p>DataLoad add Itrable ability</p>
<p>we mainly use TorchVision package of  FashionMNIST dataset dataset.pytorch offer domain-specific libraryies.</p>
<p>In this <code>Dataset</code> includes two arguments: <code>transform</code> and <code>target_transform</code> to modify the samples and labels respectively.</p>
<h5 id="dataset--dataloaders">DataSet &amp; DataLoaders<a class="td-heading-self-link" href="#dataset--dataloaders" aria-label="Heading self-link"></a></h5>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><h5 id="tensors">Tensors<a class="td-heading-self-link" href="#tensors" aria-label="Heading self-link"></a></h5>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" /><p>Pytorch use an unique data type called <em><strong>Tensors</strong></em>, similar to a multidimensional array, used to store and caculate input and output of a model.
a important feature is</p>
<blockquote>
<p>tensors can run on GPUs to accelerate computing.</p></blockquote>
<h5 id="graphs">Graphs<a class="td-heading-self-link" href="#graphs" aria-label="Heading self-link"></a></h5>
<meta name="baidu-site-verification" content="codeva-23xblvQ4Jb" />
</div>



    
	
  

    
	
  



          </main>
        </div>
      </div>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="User mailing list" aria-label="User mailing list">
    <a target="_blank" rel="noopener" href="https://example.org/mail" aria-label="User mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Twitter" aria-label="Twitter">
    <a target="_blank" rel="noopener" href="https://example.org/twitter" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Stack Overflow" aria-label="Stack Overflow">
    <a target="_blank" rel="noopener" href="https://example.org/stack" aria-label="Stack Overflow">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="GitHub" aria-label="GitHub">
    <a target="_blank" rel="noopener" href="https://github.com/hamhuo-hub/seaku" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://example.org/slack" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Developer mailing list" aria-label="Developer mailing list">
    <a target="_blank" rel="noopener" href="https://example.org/mail" aria-label="Developer mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <span class="td-footer__copyright">&copy;
    2025&ndash;2025
    <span class="td-footer__authors">海阔知识库 | <a href="https://creativecommons.org/licenses/by/4.0">CC BY 4.0</a> |</span></span><span class="td-footer__all_rights_reserved">All Rights Reserved</span><span class="ms-2"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></span>
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/js/main.min.90902a026ed2aee7ed45d85797f4c790b240ba148ac6f8a6f194cb1981ffecf5.js" integrity="sha256-kJAqAm7SruftRdhXl/THkLJAuhSKxvim8ZTLGYH/7PU=" crossorigin="anonymous"></script>
<script defer src="/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js" integrity="sha256-c0eKfUgHaYrtfjVesj&#43;YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin="anonymous"></script>
<script src='/js/tabpane-persist.js'></script>

  </body>
</html>
